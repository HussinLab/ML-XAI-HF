{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d218e62",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(xgboost)\n",
    "library(tidyverse)\n",
    "library(caret)\n",
    "library(e1071)\n",
    "library(dplyr)\n",
    "library(iml)\n",
    "\n",
    "data <- read.csv(\"## Provide data path here ##\")\n",
    "seed_values <- c(124,125,126)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c2584",
   "metadata": {},
   "source": [
    "*** SVM Model ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41671ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- read.csv(\"## Provide data path here ##\")\n",
    "\n",
    "seed_values <- c(124,125,126)   # Seed values for reproducibility\n",
    "kernel_type <- \"radial\"         # Define the SVM kernel type (can be changed)\n",
    "\n",
    "# Create an empty data frame to store misclassified sample information\n",
    "df_results_misclassified <- data.frame()\n",
    "\n",
    "# Loop through each seed value to train and test the SVM models\n",
    "for (seed in seed_values) {\n",
    "  print(seed)  # Display the current seed value\n",
    "  set.seed(seed)  # Set the seed for reproducibility\n",
    "  \n",
    "  # Create stratified folds for cross-validation\n",
    "  folds <- createFolds(factor(data$Label), k = 5) \n",
    "\n",
    "  # Initialize an empty data frame to store performance results\n",
    "  df_results <- data.frame()\n",
    "\n",
    "  # Iterate through each fold to train and evaluate the model\n",
    "  for (fold_index in 1:length(folds)) {\n",
    "    fold_indice_name <- names(folds[fold_index])  # Get the name of the current fold\n",
    "\n",
    "    # Get the testing set indices for the current fold\n",
    "    testing_indices <- folds[[fold_indice_name]]\n",
    "\n",
    "    # Create a dataframe for the test samples (sample ID and labels)\n",
    "    testing_samples <- data[testing_indices, names(data) %in% c(\"Sample\",\"Label\")]\n",
    "    rownames(testing_samples) <- NULL  # Reset rownames for testing samples\n",
    "\n",
    "    # Prepare training data excluding the testing indices\n",
    "    model_trainingset <- data[-testing_indices, ]\n",
    "    model_trainingset_labels <- model_trainingset$Label  # Extract training labels\n",
    "    model_trainingset <- model_trainingset[,!names(model_trainingset) %in% c(\"Sample\",\"Label\")]  # Remove sample ID and label columns\n",
    "\n",
    "    # Hyperparameter Tuning for SVM\n",
    "    tc = tune.control(cross = 5)  # Use 5-fold cross-validation for tuning\n",
    "    svm_tune = tune(\n",
    "      svm, train.x = model_trainingset, train.y = model_trainingset_labels,\n",
    "      kernel = kernel_type,           # Specify kernel type (e.g., radial)\n",
    "      tunecontrol = tc,               # Use the defined tune control\n",
    "      ranges = list(                  # Define ranges for cost and gamma parameters\n",
    "        cost = c(seq(7, 9, by = 0.025)),\n",
    "        gamma = outer(1:9, 10^(-4:-5)) %>% t() %>% as.vector()\n",
    "      )\n",
    "    )\n",
    "\n",
    "    # Extract the best hyperparameters from the tuning results\n",
    "    best_cost = svm_tune$best.parameters$cost\n",
    "    best_gamma = svm_tune$best.parameters$gamma\n",
    "\n",
    "    # Train the SVM model with the best hyperparameters\n",
    "    svm_model = svm(\n",
    "      x = model_trainingset, y = model_trainingset_labels, \n",
    "      type = \"nu-classification\",      # Specify SVM type\n",
    "      kernel = kernel_type,            # Use the specified kernel\n",
    "      probability = TRUE,              # Enable probability estimates\n",
    "      gamma = best_gamma,              # Use the best gamma parameter\n",
    "      cost = best_cost,                # Use the best cost parameter\n",
    "      cross = 0,                       # No additional cross-validation\n",
    "      scale = F                        # Do not scale features\n",
    "    )\n",
    "\n",
    "    # Prepare the testing set for evaluation\n",
    "    model_testingset <- data[testing_indices, ]\n",
    "    model_testingset_labels <- data.matrix(model_testingset$Label)  # Convert labels to matrix format\n",
    "    model_testingset <- model_testingset[,!names(model_testingset) %in% c(\"Sample\",\"Label\")]  # Remove sample ID and label columns\n",
    "\n",
    "    # Predict the testing set using the trained SVM model\n",
    "    cat(\"\\n RESULTS ON TESTING SET : \\n\")\n",
    "    class_model_pred <- predict(svm_model, model_testingset, type = \"raw\")  # Generate predictions\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a4da76",
   "metadata": {},
   "source": [
    "*** XGB Model ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e49c82",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create an empty data frame to store misclassified results\n",
    "df_results_misclassified <- data.frame()\n",
    "\n",
    "for (seed in seed_values) {\n",
    "\tset.seed(seed)\n",
    "\n",
    "\tfolds <-  createFolds(factor(data$Label), k = 5) #Create 5 folds - stratified\n",
    "\tdf_results <- data.frame()\n",
    "    \n",
    "\tfor (fold_index in 1:length(folds)) {\n",
    "\n",
    "\t\tfold_indice_name <-  names(folds[fold_index])  \n",
    "\n",
    "\t\ttesting_indices <-  folds[[fold_indice_name]] # Indices in data\n",
    "\t\tmodel_filename <-  paste0(fold_indice_name,\"_XGBoost_ClasseHealthyvsHFOnly_Model.rds\") \n",
    "\n",
    "\t\ttesting_samples <-data[testing_indices, names(data) %in% c(\"Sample\",\"Label\")] #Get samples id + label\n",
    "\t\trownames(testing_samples) <- NULL #remove wrong rownames id\n",
    "\n",
    "\t\tfold_filename <- paste0(fold_indice_name,\"_XGBoost_ClasseHealthyvsHFOnly_FoldSamplesList_TestingSet_Seed\",seed,\".csv\") \n",
    "\t\t\n",
    "\t\tmodel_trainingset <-  data[-testing_indices, ] # Make training set from the testing folds indices\n",
    "\t\tmodel_trainingset_labels <-  data.matrix(model_trainingset$Label) # True labels from the training set as matrix\n",
    "\t\tmodel_trainingset <-  model_trainingset[,!names(model_trainingset) %in% c(\"Sample\",\"Label\")] # Remove Sample id and Labels from the training set\n",
    "\t\tmodel_trainingset <-  data.matrix(model_trainingset) # Transform dataframe as matrix so it can be used with xgb library\n",
    "\n",
    "\t\tXGB_dtrain <-  xgb.DMatrix(data = model_trainingset, label= model_trainingset_labels) # Object from XGB library.\n",
    "\n",
    "\n",
    "\t\txgb_grid_1 <-  expand.grid(\n",
    "\t\t\tnrounds = 1000,\n",
    "\t\t\teta = c(0, 0.2, 0.4, 0.6, 0.8, 1.0),\n",
    "\t\t\tmax_depth = c(1,5,10,15,20),\n",
    "\t\t\tgamma = c(0,1,2,5,10),               #default=0\n",
    "\t\t\tcolsample_bytree = 1,    #default=1 step=0.1\n",
    "\t\t\tmin_child_weight = c(0,0.5,1,2,5),     #default=1, step=0.1, Intuitively, this is the minimum number of samples that a node can represent in order to be split further. If there are fewer than min_child_weight samples at that node, the node becomes a leaf and is no longer split. This can help reduce the model complexity and prevent overfitting.\n",
    "\t\t\tsubsample = 1    #default=1\n",
    "\t\t)\n",
    "\n",
    "\t\txgb_trcontrol_1 <-  trainControl(\n",
    "\t\t\tmethod = \"cv\",\n",
    "\t\t\tnumber = 5,\n",
    "\t\t\tverboseIter = FALSE,\n",
    "\t\t\treturnData = FALSE,\n",
    "\t\t\treturnResamp = \"all\",                                                        # save losses across all models\n",
    "\t\t\tclassProbs = TRUE,                                                           # set to TRUE for AUC to be computed\n",
    "\t\t\tallowParallel = TRUE\n",
    "\t\t)\n",
    "\n",
    "# Train the model : XGBoost\n",
    "\t\txgb_train_1 <-  train(\n",
    "\t\t\tx = model_trainingset,\n",
    "\t\t\ty = make.names(model_trainingset_labels),\n",
    "\t\t\ttrControl = xgb_trcontrol_1,\n",
    "\t\t\ttuneGrid = xgb_grid_1,\n",
    "\t\t\tmethod = \"xgbTree\"\n",
    "\t\t)\n",
    "\n",
    "\t\tbest_nrounds <-  xgb_train_1$bestTune$nrounds \n",
    "\t\tbest_max_depth <-  xgb_train_1$bestTune$max_depth\n",
    "\t\tbest_eta <-  xgb_train_1$bestTune$eta\n",
    "\t\tbest_gamma <-  xgb_train_1$bestTune$gamma\n",
    "\t\tbest_colsample <-  xgb_train_1$bestTune$colsample_bytree\n",
    "\t\tbest_min_child <-  xgb_train_1$bestTune$min_child_weight\n",
    "\t\tbest_subsample <-  xgb_train_1$bestTune$subsample\n",
    "\n",
    "\n",
    "\t\tdf_hp <- data.frame(nround= best_nrounds,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmax_depth=best_max_depth,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\teta=best_eta,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tgamma=best_gamma,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcolsample=best_colsample,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmin_child=best_min_child,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsub_sample=best_subsample)\n",
    "\t\n",
    "\n",
    "\t\txgb_model <-  xgboost(data = XGB_dtrain, # the data   \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tnrounds = best_nrounds,# max number of boosting iterations\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\teta = best_eta, #learning rate 0.001\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tmax_depth = best_max_depth, #max deppth of trees\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tgamma =best_gamma,               #default=0\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tcolsample_bytree = best_colsample,    #default=1, is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tmin_child_weight = best_min_child,     #default=1, Intuitively, this is the minimum number of samples that a node can represent in order to be split further. If there are fewer than min_child_weight samples at that node, the node becomes a leaf and is no longer split. This can help reduce the model complexity and prevent overfitting.\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tsubsample = best_subsample,     #default=1\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tearly_stopping_rounds = 5,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tobjective = \"binary:logistic\"\n",
    "\t\t) \n",
    "\n",
    "\n",
    "\t\tmodel_testingset <-  data[testing_indices, ] #Make training set from the testing folds indices.\n",
    "\t\tmodel_testingset_labels <-  data.matrix(model_testingset$Label) #True labels of testing set\n",
    "\t\tmodel_testingset <-  model_testingset[,!names(model_testingset) %in% c(\"Sample\",\"Label\")] #Remove labels and ids\n",
    "\t\tmodel_testingset <-  data.matrix(model_testingset) #transform dataframe as matrix so it can be used with xgb library\n",
    "\n",
    "\t\tXGB_dtest <- xgb.DMatrix(data = model_testingset, label= model_testingset_labels)\n",
    "\t\tmodel_pred <-  predict(xgb_model, XGB_dtest,type=\"response\") #prediction of the testing set using corresponding XGB model\n",
    "\n",
    "\t\txgb_pred_class = as.numeric(model_pred > 0.50)\n",
    "\t\txgb_pred_class\n",
    "\n",
    "\t\t\n",
    "\n",
    "\t\t}\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883263e6",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "*** LIME ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb30897",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "LIME_output_all <-  function (path_dataset,seed_values,models_folder,model_kind, n_features,folder_to_save_results_files=paste0(models_folder,\"LIME_Analysis/\")) {\n",
    "\n",
    "  dataset <- read.csv(path_dataset,check.names=FALSE,sep=\",\",colClasses=c(\"Label\"=\"factor\"))\n",
    "  row.names(dataset) <- dataset$Sample\n",
    "  \n",
    "\tfor (seed in seed_values) {\n",
    "\t\tset.seed(seed)\n",
    "\t\tlist_testingset_files <- list.files(path=\".\",pattern = paste0(\"*_TestingSet_Seed\",seed,\".csv\"))\n",
    "\t\t\n",
    "\t\tfor (i in 1:length(list_testingset_files))  {\n",
    "\t\t\ttestingset_sample_ids_from_saved_fold_file <- read.csv(list_testingset_files[i],sep=\"\\t\")\n",
    "\t\t\t\n",
    "\t\t\tmodel_name <- paste0(\"Fold\",i,\"_\",model_kind,\"_ClasseHealthyvsHFOnly_Model.rds\") \n",
    "\t\t\tmodel <- readRDS(model_name, refhook = NULL)\n",
    "\n",
    "\t\t\t\t\n",
    "\t\t\tmodel_trainingset <- dataset %>% tibble::rownames_to_column(\"sample_ids\") %>% \n",
    "\t\t\t\tfilter(!Sample %in% testingset_sample_ids_from_saved_fold_file$Sample) %>%\n",
    "\t\t\t\ttibble::column_to_rownames(\"sample_ids\")\n",
    "\t\t\tmodel_trainingset_labels <- model_trainingset$Label\n",
    "\t\t\tmodel_trainingset <-  model_trainingset[,!names(model_trainingset) %in% c(\"Sample\",\"Label\")] #Remove Sample id and Labels from the training set\n",
    "\t\t\t\n",
    "\t\t\tmodel_testingset <- dataset %>% tibble::rownames_to_column(\"sample_ids\") %>%\n",
    "\t\t\t\tfilter(Sample %in% testingset_sample_ids_from_saved_fold_file$Sample) %>%  #Make training set from the testing folds indices. NB : use -X TO GET TRAIN SET\n",
    "\t\t\t\ttibble::column_to_rownames(\"sample_ids\")\n",
    "\t\t\tmodel_testingset_labels <-  model_testingset$Label #True labels of testing set\n",
    "\t\t\tmodel_testingset <-  model_testingset[,!names(model_testingset) %in% c(\"Sample\",\"Label\")] #Remove labels and ids\n",
    "\t\t\t\n",
    "\t\t\tmodel_trainingset <- model_testingset \n",
    "\t\t\tmodel_trainingset_labels <- model_testingset_labels\n",
    "\t\t\t\t\n",
    "\t\t\tif(model_kind==\"Logit\"){\n",
    "\t\t\t\tused_feature_tofit <- model$terms %>% attr(.,\"dataClasses\") %>% names(.)\n",
    "\t\t\t\tused_feature_tofit <- used_feature_tofit[used_feature_tofit != \"Label\"]\n",
    "\t\t\t\tmodel_testingset <- model_testingset[,names(model_testingset) %in% used_feature_tofit]\n",
    "\t\t\t\tmodel_trainingset <- model_trainingset[,names(model_trainingset) %in% used_feature_tofit]\n",
    "\t\t\t}\n",
    "\t\t\t\n",
    "\t\t\tfor (n_set in 1:2)  { #2 : training_set part, 1:testing_set part\n",
    "\t\t\t\t\n",
    "\t\t\t\texplainer_model <-  lime(model_trainingset,model) #should be the training set\n",
    "\t\t\t\t\n",
    "\t\t\t\tif (n_set==1) {\n",
    "\t\t\t\t\tcat(\"\\t explanation_model for testing set\")\n",
    "\t\t\t\t\texplanation_model <-  explain(\n",
    "\t\t\t\t\t\tx =  model_testingset, \n",
    "\t\t\t\t\t\texplainer =  explainer_model, \n",
    "\t\t\t\t\t\tn_permutations =  5000,\n",
    "\t\t\t\t\t\tdist_fun =  \"gower\",\n",
    "\t\t\t\t\t\tkernel_width =  NULL, \n",
    "\t\t\t\t\t\tn_features = n_features,#n_features, #top 10\n",
    "\t\t\t\t\t\tfeature_select =  \"auto\",\n",
    "\t\t\t\t\t\tlabels =  c(\"1\"))\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t} else {\n",
    "\t\t\t\t\tcat(\"\\t explanation_model for training set\")\n",
    "\t\t\t\t\texplanation_model <-  explain(\n",
    "\t\t\t\t\t\tx =  model_trainingset, \n",
    "\t\t\t\t\t\texplainer =  explainer_model, \n",
    "\t\t\t\t\t\tn_permutations =  5000,\n",
    "\t\t\t\t\t\tdist_fun =  \"gower\",\n",
    "\t\t\t\t\t\tkernel_width =  NULL,\n",
    "\t\t\t\t\t\tn_features = n_features,#n_features, #top 10\n",
    "\t\t\t\t\t\tfeature_select =  \"auto\",\n",
    "\t\t\t\t\t\tlabels =  c(\"1\")) \n",
    "\t\t\t\t}\n",
    "\t\t\t\t}\n",
    "\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea38159",
   "metadata": {},
   "source": [
    "*** Feature Interaction : H-Friedman ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b2e3e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ia_results_list <- list()\n",
    "\n",
    "n_iterations <- 100  # replace with the number of iterations you want to run\n",
    "\n",
    "for (i in 1:n_iterations) {\n",
    "  mod <- iml::Predictor$new(your_model,data = newdata_test,y=model_testingset_xgb_labels,predict.fun=predict.fun,class=\"1\")\n",
    "  ia <- Interaction$new(mod)\n",
    "\n",
    "  # add a column to ia$results that indicates the iteration number\n",
    "  ia$results$iteration <- i\n",
    "  \n",
    "  # store the ia$results in the list\n",
    "  ia_results_list[[i]] <- ia$results\n",
    "}\n",
    "\n",
    "# bind all the ia$results data frames into a single data frame\n",
    "interaction_df <- bind_rows(ia_results_list)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
